% Emacs, this is -*-latex-*-

\title{\href{https://github.com/vicente-gonzalez-ruiz/vector_quantization}{Vector Quantization (VQ)}}

\maketitle
\tableofcontents

\section{Overview}
%{{{

In VQ, samples are quantized in groups (\emph{vectors}), producing a
quantization index by vector~\cite{sayood2017introduction}.  Usually,
the lengths of the quantization indexes are much shorter than the
lengths of the vectors, generating the data compression. However, it
is possible also to use an entropy codec con further compress the
quantization indexes.
%}}}

\section{Vector Quantization vs Scalar Quantization}

Vector Quantization (VQ) can remove auto-correlation in the encoded
signal and therefore, is more efficient in RD
terms~\cite{vruiz__information_theory} than Scalar Quantization (SQ).

%Unfortunately, the
%computational requirements of VQ are, by far, much higher than the
%needed by SQ. If we also consider that there are other techniques
%(such as transform coding, that we will see later) that are able to
%decorrelate the samples requiring less computational resources than
%VQ, we can understand why SQ has been selected, for example, in most
%\href{https://en.wikipedia.org/wiki/Image_compression}{image} and
%\href{https://en.wikipedia.org/wiki/Video_coding_format}{video
%  codecs}.

%}}}

\section{Encoding and decoding}
%{{{

The VQ inputs blocks of $L$ elements (samples of speech, pixels of an
image, etc.), usually using an adaptive algorithm, build a
\emph{code-book} (a set of $L$-dimensional vectors called
\emph{code-vectors} which are selected to be representative of the
input), compare the input vectors with all the code-vectors, and
outputs the index of the code-vector that minimizes some distortion
measurement. In other words, if $\mathbf{x}$ is an input vector and
$\mathbf{y}_j$ is the selected code-vector (of the code-book
$\mathcal{C}=\{\mathbf{y}_i;i=0,1,\cdots,K-1\}$), it is satisfied that
\begin{equation}
  ||\mathbf{x}-\mathbf{y}_j||^2 \le || \mathbf{x}-\mathbf{y}_j||^2\qquad\forall\mathbf{y_i}\in\mathcal{C}.
\end{equation}

Because the build exactly the same codebook, it can retrieve the
decompressed code-vector given its quantization index. Notice that,
although the encoder likely have to perform a considerable amount of
computations in order to build the codebook and to find the closest
code-vector, the decoding consists simply of a table lookup.

If $K$ is the number of code-vectors in the code-book and the input is
splitted in vectors of length $L$, the quantizer will use
$\lceil\log_2 K\rceil=S$ bits per input vector. Therefore, the number
of bits/sample would be $S/L$.

%}}}

\section{Code-book design}
%{{{

If the source output is correlated, vectors of source output values
will tend to fall in clusters~\cite{sayood2017introduction}. The idea
is to put in $\mathcal{C}$ a set of code-vectors that minimize the
distortion of VQ. In other words, we must split the signal space (of
dimenssion $L$) in a set of $K$ regions
\begin{equation}
  V_k=\{\mathbf{x}:d(\mathbf{x},\mathbf{y}_j) <
  d(\mathbf{x},\mathbf{y}_i), \forall i\ne j\},
\end{equation}
of arbitrary shape.


This is a combinatorial optimization problem in a rather large search
space, which usually makes it impossible to determine a global optimum
in adequate time. This is the reason why VQ methods only compute a
``local'' optimum at best~\cite{burger2016digital}. Anyway, VQ is
used, for example, in
\href{https://en.wikipedia.org/wiki/Palette_(computing)}{``palletized''
  images}.
  

\subsection{K-means}

If we know $K$ (the size of the code-book $\mathcal{C}$), we can find
$\mathcal{C}$) using the K-means
algorithm~\cite{hartigan1979algorithm,sayood2017introduction}. This
algorithm computest$\mathcal{C}$ as the set of the centroid of each
region $V_k$.

\subsection{The Linde, Buzo, and Gray (LGB) Algorithm}

The LGB Algorithm~\cite{linde1980algorithm} is a generalization of the
Lloyd Algorithm~\cite{lloyd1982least}, where $K$ is estimated
from the input vectors and then, the K-means Algorithm is used to
compute the $L$-dimensional centroids of $\mathcal{C}$.

%}}}

\section{Vector Quantization of Gray-scale Images}

%{{{

The compression ratio (and distortion) of our lossy image compressor
can be controled by means of quantization. At this point we have
basically two alternatives:
\begin{enumerate}
\item \textbf{Scalar Quantization (SQ)}: where each pixel is quantized
  without considering the rest of pixels in the
  image~\cite{vruiz__scalar_quantization}.
\item \textbf{Vector Quantization (VQ)}: when the pixels are quantized
  block-by-block (2D vectors)~\cite{vruiz__vector_quantization}.
\end{enumerate}
Notice that VQ exploits the spatial correlation, but SQ doesn't. For
this reason, by default, we will use PNG after SQ, because PNG can
remove the spatial redundancy. In the case of VQ, only the statistical
redundancy remains, that can be exploited by any entropy codec (and
obviously, any image codec).

If we can found 2D redundancy in an gray-scale
image~\cite{vruiz__image_IO}, Vector Quantization
(VQ)~\cite{vruiz__vector_quantization} applied to the spatial domain
of images can provide better RD curves than Scalar Quantization
(SQ)~\cite{vruiz__scalar_quantization}. After using VQ on the image we
will obtain an matrix of quantization indexes and we can use PNG as an
entropy codec to remove the statistical redundancy.\footnote{Notice
that PNG could also remove the remaining spatial redundancy, but we
can expect that only statistical redundancy can be found in the
sequence of quantization indexes.} Let's denote such image codec by
VQ+PNG.

%}}}

%{{{

If VQ+PNG is used to compress an image, we must realize that:
\begin{enumerate}
\item The rate of the code-stream (for example, the number of
  bits/pixels obtained after compressing) depends on the size $L$ of
  the vectors (usually squared blocks of pixels) and number the $K$ of
  different vectors that we consider in the
  code-book~\cite{vruiz__vector_quantization}. Notice that, without
  considering PNG, we will generate $\lceil\log_2 K\rceil$ bits per
  quantization index, and one quantization index will be generated
  each $L$ input pixels.\footnote{Resulting in a quantizer's rate of
  $\frac{\lceil\log_2 K\rceil}{L}$bits per pixel.} Hopefully, after
  using PNG on the indexes, this number of bits/vector will be futher
  reduced.
\item The distortion generated by VQ depends on $L$, $K$, but also
  depends on:
  \begin{enumerate}
  \item The hability of VQ to chose the best vectors that will be used
    in the code-stream. Different algorithms will provide different RD
    curves~\cite{vruiz__information_theory}.
  \item The content of the input image. For example, images with
    complex textures will require, in general, smaller vectors or
    larger code-books.
  \end{enumerate}
\end{enumerate}

%}}}

\section{Resources}

\renewcommand{\addcontentsline}[3]{} % Remove functionality of \addcontentsline
\bibliography{data-compression,signal-processing,DWT,image-processing,information_theory,pattern_recognition,quantization}
