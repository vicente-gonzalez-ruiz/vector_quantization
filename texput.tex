% Emacs, this is -*-latex-*-

\title{\href{https://github.com/vicente-gonzalez-ruiz/vector_quantization}{Vector Quantization (VQ)}}

\maketitle
\tableofcontents

\section{Overview}
%{{{

In VQ, samples are quantized in groups (\emph{vectors}), producing a
quantization index by vector~\cite{sayood2017introduction}.  Usually,
the lengths of the quantization indexes are much shorter than the
lengths of the vectors, generating the data compression. However, it
is possible also to use an entropy codec con further compress the
quantization indexes.
%}}}

\section{Vector Quantization vs Scalar Quantization}

Vector Quantization (VQ) can remove auto-correlation in the encoded
signal and therefore, is more efficient in RD
terms~\cite{vruiz__information_theory} than Scalar Quantization (SQ).

%Unfortunately, the
%computational requirements of VQ are, by far, much higher than the
%needed by SQ. If we also consider that there are other techniques
%(such as transform coding, that we will see later) that are able to
%decorrelate the samples requiring less computational resources than
%VQ, we can understand why SQ has been selected, for example, in most
%\href{https://en.wikipedia.org/wiki/Image_compression}{image} and
%\href{https://en.wikipedia.org/wiki/Video_coding_format}{video
%  codecs}.

%}}}

\section{Encoding and decoding}
%{{{

The VQ inputs blocks of $L$ elements (samples of speech, pixels of an
image, etc.), usually using an adaptive algorithm, build a
\emph{code-book} (a set of $L$-dimensional vectors called
\emph{code-vectors} which are selected to be representative of the
input), compare the input vectors with all the code-vectors, and
outputs the index of the code-vector that minimizes some distortion
measurement. In other words, if $\mathbf{x}$ is an input vector and
$\mathbf{y}_j$ is the selected code-vector (of the code-book
$\mathcal{C}=\{\mathbf{y}_i;i=0,1,\cdots,K-1\}$), it is satisfied that
\begin{equation}
  ||\mathbf{x}-\mathbf{y}_j||^2 \le || \mathbf{x}-\mathbf{y}_j||^2\qquad\forall\mathbf{y_i}\in\mathcal{C}.
\end{equation}

Because the build exactly the same codebook, it can retrieve the
decompressed code-vector given its quantization index. Notice that,
although the encoder likely have to perform a considerable amount of
computations in order to build the codebook and to find the closest
code-vector, the decoding consists simply of a table lookup.

If $K$ is the number of code-vectors in the code-book and the input is
splitted in vectors of length $L$, the quantizer will use
$\lceil\log_2 K\rceil=S$ bits per input vector. Therefore, the number
of bits/sample would be $S/L$.

%}}}

\section{Code-book design}
%{{{

If the source output is correlated, vectors of source output values
will tend to fall in clusters~\cite{sayood2017introduction}. The idea
is to put in $\mathcal{C}$ a set of code-vectors that minimize the
distortion of VQ. In other words, we must split the signal space (of dimenssion $L$) in a set of $K$ regions
\begin{equation}
  V_k=\{\mathbf{x}:d(\mathbf{x},\mathbf{y}_j) <
  d(\mathbf{x},\mathbf{y}_i), \forall i\ne j\},
\end{equation}
of arbitrary shape.

\subsection{K-means}

If we know $K$ (the size of the code-book $\mathcal{C}$), we can find
$\mathcal{C}$) using the K-means
algorithm~\cite{hartigan1979algorithm,sayood2017introduction}. This
algorithm computest$\mathcal{C}$ as the set of the centroid of each
region $V_k$.

\subsection{The Linde, Buzo, and Gray (LGB) Algorithm}

The LGB Algorithm~\cite{linde1980algorithm} is a generalization of the
Lloyd Algorithm~\cite{lloyd1982least}, where $K$ is estimated
from the input vectors and then, the K-means Algorithm is used to
compute the $L$-dimensional centroids of $\mathcal{C}$.

%}}}

\section{Resources}

\renewcommand{\addcontentsline}[3]{} % Remove functionality of \addcontentsline
\bibliography{data-compression,signal-processing,DWT,image-processing,information_theory,pattern_recognition}
